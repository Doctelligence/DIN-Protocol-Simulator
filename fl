import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.3
config.gpu_options.visible_device_list = "1"
config.allow_soft_placement=True
config.log_device_placement=True
config.gpu_options.allocator_type = 'BFC'
set_session(tf.Session(config=config))

from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger

from keras.models import Model
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import VGG16
import keras.metrics
from keras.optimizers import Adam, RMSprop
import numpy as np


from sklearn.metrics import classification_report, confusion_matrix
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve

from sklearn.metrics import auc


#import matplotlib.pyplot as plt


import numpy, scipy.io

#Personal Health Record data

def split_iid(dataset, n_centers):
    """ Split PyTorch dataset randomly into n_phr """
    n_obs_per_phr = [len(dataset) // n_phr for _ in range(n_phr)]
    return random_split(dataset, n_obs_per_phr)
    
def federated_averaging(models, n_obs_per_client):
    assert len(models) > 0, 'An empty list of models was passed.'
    assert len(n_obs_per_client) == len(models), 'List with number of observations must have ' \
                                                 'the same number of elements that list of models.'

    # Compute proportions
    n_obs = sum(n_obs_per_client)
    proportions = [n_k / n_obs for n_k in n_obs_per_client]

    # Empty model parameter dictionary
    avg_params = models[0].state_dict()
    for key, val in avg_params.items():
        avg_params[key] = torch.zeros_like(val)

    # Compute average
    for model, proportion in zip(models, proportions):
        for key in avg_params.keys():
            avg_params[key] += proportion * model.state_dict()[key]

    # Copy one of the models and load trained params
    avg_model = copy.deepcopy(models[0])
    avg_model.load_state_dict(avg_params)

    return avg_model    

def create_keras_model():
  return tf.keras.models.Sequential([
      tf.keras.layers.InputLayer(input_shape=(784,)),
      tf.keras.layers.Dense(10, kernel_initializer='zeros'),
      tf.keras.layers.Softmax(),
  ])
  
  def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=personal_health_record,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
      
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))      
    
str(iterative_process.initialize.type_signature)

state = iterative_process.initialize()

state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

NUM_ROUNDS = 11
for round_num in range(2, NUM_ROUNDS):
  state, metrics = iterative_process.next(state, federated_train_data)
  print('round {:2d}, metrics={}'.format(round_num, metrics))
  
  #Declare model
model_ =keras.applications.resnet.ResNet50(include_top=True, weights='imagenet')

#Add a layer where input is the output of the  second last layer 
x = Dense(2, activation='softmax', name='predictions')(model_.layers[-2].output)
model = Model(input=model_.input, output=x)
model.summary()


metrics = ['accuracy']
optimizer = Adam(lr=1e-5, decay=1e-6)
steps_per_epoch = (28526) // batch_size

# Helper: Stop when we stop learning.
early_stopper = EarlyStopping(patience=2)
    
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,metrics=metrics)
#model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,metrics=metrics)

# Train the model
model.fit_generator(train_it, 
epochs=5,
#steps_per_epoch=steps_per_epoch,
validation_data=test_it,
callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)],
verbose=1, 
workers=1,
validation_steps=40)

# evaluate model
#loss = model.evaluate_generator(test_it, steps=24)

#Confution Matrix and Classification Report
Y_pred = model.predict_generator(test_it, 5480 // batch_size+1)
y_pred = np.argmax(Y_pred, axis=1)
print('Confusion Matrix')
tn, fp, fn, tp =confusion_matrix(test_it.classes, y_pred).ravel()
print(tp)
print(tn)
print(fp)
print(fn)
specificity = tn / (tn+fp)
print(specificity)
print('Classification Report')
target_names = ['Negative', 'Positive']
print(classification_report(test_it.classes, y_pred, target_names=target_names))


# ROC
#from sklearn.metrics import roc_curve
#y_pred_keras = model.predict(test_it)
#fpr_keras, tpr_keras, thresholds_keras = roc_curve(test_it, y_pred_keras)

#from sklearn.metrics import auc
#auc_keras = auc(fpr_keras, tpr_keras)

#print(fpr_keras)
#print(tpr_keras)

#scipy.io.savemat('roc_vgg16_mlp.mat', mdict={'fpr': fpr_keras,'tpr': tpr_keras})


# ROC Curves
#Y_pred = model.predict_generator(test_it, 5480 // batch_size).ravel()
#fpr_rf, tpr_rf, thresholds_rf = roc_curve(test_it.classes, y_pred)
#auc_rf = auc(fpr_rf, tpr_rf)

#scipy.io.savemat('roc_vgg16_mlp.mat', mdict={'fpr': fpr_rf,'tpr': tpr_rf})
